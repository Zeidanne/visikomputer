# =================================================
# Program : Decision Tree - Play Golf
# =================================================

import pandas as pd
import numpy as np
from math import log2
from pprint import pprint
from sklearn import tree
import matplotlib.pyplot as plt

# -------------------------------
# 1. Membuat Data
# -------------------------------

data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny',
                'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild',
                    'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High',
                 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Windy': ['False', 'False', 'True', 'False', 'False', 'True', 'True', 'False',
              'False', 'False', 'True', 'True', 'False', 'True'],
    'PlayGolf': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No',
                 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

df = pd.DataFrame(data)
print("Data Awal:\n", df, "\n")

# -------------------------------
# 2. Fungsi untuk menghitung Entropy
# -------------------------------

def entropy(target_col):
    elements, counts = np.unique(target_col, return_counts=True)
    entropy_value = 0
    for i in range(len(elements)):
        probability = counts[i] / np.sum(counts)
        entropy_value -= probability * log2(probability)
    return round(entropy_value, 3)

# -------------------------------
# 3. Fungsi untuk menghitung Information Gain
# -------------------------------

def info_gain(data, split_attribute_name, target_name="PlayGolf"):
    total_entropy = entropy(data[target_name])
    vals, counts = np.unique(data[split_attribute_name], return_counts=True)
    
    weighted_entropy = 0
    for i in range(len(vals)):
        subset = data[data[split_attribute_name] == vals[i]]
        weighted_entropy += (counts[i] / np.sum(counts)) * entropy(subset[target_name])
    
    information_gain = total_entropy - weighted_entropy
    return round(information_gain, 3)

# -------------------------------
# 4. Membentuk Decision Tree secara manual (rekursif)
# -------------------------------

def Id3(data, originaldata, features, target_attribute_name="PlayGolf", parent_node_class=None):
    # Jika semua target bernilai sama -> return nilai itu
    if len(np.unique(data[target_attribute_name])) <= 1:
        return np.unique(data[target_attribute_name])[0]

    # Jika data kosong -> return nilai mayoritas dari data asli
    elif len(data) == 0:
        return np.unique(originaldata[target_attribute_name])[
            np.argmax(np.unique(originaldata[target_attribute_name], return_counts=True)[1])
        ]

    # Jika fitur kosong -> return nilai mayoritas
    elif len(features) == 0:
        return parent_node_class

    # Jika tidak, lanjutkan
    else:
        parent_node_class = np.unique(data[target_attribute_name])[
            np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])
        ]
        
        # Hitung gain untuk tiap atribut
        gains = [info_gain(data, feature, target_attribute_name) for feature in features]
        best_feature_index = np.argmax(gains)
        best_feature = features[best_feature_index]

        # Bangun pohon
        tree_structure = {best_feature: {}}

        # Buang fitur terbaik dari daftar
        remaining_features = [f for f in features if f != best_feature]

        for value in np.unique(data[best_feature]):
            sub_data = data[data[best_feature] == value]
            
            subtree = Id3(sub_data, data, remaining_features, target_attribute_name, parent_node_class)
            tree_structure[best_feature][value] = subtree
    
    return tree_structure

# -------------------------------
# 5. Bangun pohon keputusan
# -------------------------------

features = list(df.columns[:-1])
decision_tree = Id3(df, df, features)
print("Hasil Pohon Keputusan (Manual ID3):")
pprint(decision_tree)

# -------------------------------
# 6. Visualisasi menggunakan sklearn
# -------------------------------

from sklearn.preprocessing import LabelEncoder

# Encode data kategori
le = LabelEncoder()
df_encoded = df.apply(le.fit_transform)

X = df_encoded.drop(columns=['PlayGolf'])
y = df_encoded['PlayGolf']

clf = tree.DecisionTreeClassifier(criterion='entropy')
clf = clf.fit(X, y)

plt.figure(figsize=(10,6))
tree.plot_tree(clf, feature_names=list(X.columns), class_names=['No', 'Yes'], filled=True)
plt.show()